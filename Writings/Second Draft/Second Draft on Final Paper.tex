\documentclass{IEEEtran}
\usepackage{cite}
\begin{document}
\title{A Survey on Neural Language Modeling Techniques}

\author{Anuar Maratkhan}

\maketitle

\section{Introduction}

A \textit{language model} is an algorithm for predicting next word in a text given preceeding ones. It is also mostly known as a \textit{statistical language model} for its richness in statistical approaches in previous works. However, as soon as neural networks became popular, a \textit{neural language modeling} approaches has been invented. The language modeling is used in speech recognition, handwriting recognition, machine translation, information retrieval and other natural language processing applications.

First, we will discuss earlier approaches to language modeling, particularly statistical ones that are \textit{unigram}, \textit{bigram}, \textit{trigram}, and more generalized \textit{N-gram} models. Further, we proceed by reviewing nowadays' state-of-the-art - neural language models. And by the end, we will summarize the topic and give future suggestions on language models.

Neural Language Models, also known as continous-space language models, make use of neural networks. These are current state-of-the-art approach in language modeling. One reason of Neural Nets' use is that they can learn from unstructured data. The studies show that it is possible to learn from unlabeled data \cite{unsupervised}. One of the common applications of such new machine learning techniques is search engines, which is mainly information retrieval that consists of unlabeled data. That means neural nets are possible to learn from that unstructured data and perform well on that.

\section{Word-Level Language Modeling}

Recurrent Neural Networks (RNN) are used in sequential data analysis such as video processing, text processing, predicting stock prices. RNN architecture is an improvement of feed-forward networks that takes into account previous output results by storing them in memory. An invention of backpropogation caused an exciting use of RNNs \cite{deeplearning}. But the problem in simple RNN was that the backpropogated gradients vanshish or explode \cite{deeplearning}.

The improvement in RNN architecture further was introduced with a Long-Short Term Memory (LSTM). The LSTM solved the vasnishing gradient problem by having special cells with activation gates. Moreover, Gated Recurrent Unit (GRU) improved the architecture of the LSTM by combining two gates. The study shows that RNNs perform better than complex statistical approaches, N-grams, on a real-world data \cite{rnn}.

\section{Subword-level Language Modeling}

In progress...

\section{Reinforcement Learning}

This section will show some Reinforcement Learning (RL) approaches to language modeling tasks and applications such as dialogue systems, machine translation, text generation.

\section{Neural Turing Machines}

This section will discuss Neural Turing Machines (NTM) approach to different natural language processing tasks with controllers like feed-forward network, GRU, LSTM, and further will show applications in learning language models. Related papers are \cite{snips16}

\section{Conclusion}

Over the past few years, it was clearly seen that continous-space language models outperformed statistical modeling techniques significantly. The ease of feature learning without need in extraction of those features simplified lives of scientists. The neural approach offers such opportunity to its users.

Recurrent Networks are expected to improve natural language understanding in real-world applications.

To be continued...


\bibliographystyle{IEEEtran}
\bibliography{reference}
\printbibliography

\end{document}