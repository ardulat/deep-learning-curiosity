% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{unsupervised}
Q.~V. Le, M.~A. Ranzato, R.~Monga, M.~Devin, K.~Chen, G.~S. Corrado, J.~Deau,
  and A.~Y. Ng., ``Building high-level features using large scale unsupervised
  learning,'' in \emph{Proceedings of the 29 th International Confer- ence on
  Machine Learning}, Edinburgh, Scotland, UK, 2012.

\bibitem{Mikolov2010NeuralLM}
T.~Mikolov, M.~Karafi{\'a}t, L.~Burget, J.~Cernock{\'y}, and S.~Khudanpur,
  ``Recurrent neural network based language model,'' in \emph{Interspeech},
  vol.~2, 2010.

\bibitem{Zaremba2014LSTM}
W.~Zaremba, I.~Sutskever, , and O.~Vinyals, ``Recurrent neural network
  regularization,'' 2014, arXiv preprint arXiv:1409.2329.

\bibitem{Ghahramani2016Dropout}
Y.~Gal and Z.~Ghahramani, ``A theoretically grounded application of dropout in
  recurrent neural networks,'' in \emph{Advances in neural information
  processing systems}, 2016, p. 1019â€“1027.

\bibitem{Inan2016TiedLSTM}
H.~Inan, K.~Khosravi, and R.~Socher, ``Tying word vectors and word classifiers:
  A loss framework for language modeling,'' 2016, arXiv preprint
  arXiv:1611.01462.

\bibitem{Salakhutdinov2017Softmax}
Z.~Yang, Z.~Dai, R.~Salakhutdinov, and W.~W. Cohen, ``Breaking the softmax
  bottleneck: A high-rank rnn language model,'' 2017, under review as a
  conference paper at ICLR 2018.

\bibitem{deeplearning}
Y.~LeCun, Y.~Bengio, and G.~Hinton, ``Deep learning,'' \emph{Macmillan
  Publishers Limited}, vol. 521, pp. 436--444, 2015.

\bibitem{rnn}
A.~Karpathy, J.~Johnson, and L.~Fei-Fei, ``Visualizing and understanding
  reccurent networks,'' 2015, under review as a conference paper at ICLR 2016.

\bibitem{Mikolov2011SubwordLM}
T.~Mikolov, I.~Sutskever, A.~Deoras, H.-S. Le, S.~Kombrink, and
  J.~Cernock{\'y}, ``Subword language modeling with neural networks,'' 2011.

\bibitem{snips16}
T.~Deleu and J.~Dureau, ``Learning operations on a stack with neural turing
  machines,'' in \emph{1st Workshop on Neural Abstract Machines & Program
  Induction (NAMPI)}, Barcelona, Spain, 2016.

\end{thebibliography}
